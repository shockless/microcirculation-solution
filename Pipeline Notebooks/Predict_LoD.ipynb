{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Импорты\n"
      ],
      "metadata": {
        "id": "UkB-5H5wP-Uo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNKjTzw8piu-",
        "outputId": "cae40f5b-1a30-4b95-c1a8-a4bdc20e0175"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tZq9BPXcG-YQkV40ryIwVqPQUpWXCk1-\n",
            "To: /content/test/test_dataset_mc2.zip\n",
            "100% 876M/876M [00:12<00:00, 69.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=12hNXcrHr0v48m9VRr-GLj5eq3pnYU7Im\n",
            "To: /content/requirements.txt\n",
            "100% 225/225 [00:00<00:00, 406kB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1tZq9BPXcG-YQkV40ryIwVqPQUpWXCk1- -O /content/test/\n",
        "!gdown 12hNXcrHr0v48m9VRr-GLj5eq3pnYU7Im\n",
        "!unzip -qq /content/test/test_dataset_mc2.zip -d /content/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r4Bwkt4pkq6",
        "outputId": "e4b60d94-2db0-498f-df1c-0bd6c09ba679"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu113\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 1)) (1.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 2)) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 3)) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 4)) (4.64.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 5)) (4.6.0.66)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
            "Collecting segmentation_models_pytorch\n",
            "  Downloading segmentation_models_pytorch-0.3.0-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 8)) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 9)) (3.2.2)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 10)) (7.9.0)\n",
            "Collecting GPUtil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 12)) (4.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 13)) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 14)) (0.13.1+cu113)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 15)) (0.12.1+cu113)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from albumentations->-r /content/requirements.txt (line 1)) (0.0.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations->-r /content/requirements.txt (line 1)) (1.7.3)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations->-r /content/requirements.txt (line 1)) (0.18.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations->-r /content/requirements.txt (line 1)) (6.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations->-r /content/requirements.txt (line 1)) (4.6.0.66)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations->-r /content/requirements.txt (line 1)) (1.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations->-r /content/requirements.txt (line 1)) (4.1.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations->-r /content/requirements.txt (line 1)) (2.9.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations->-r /content/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations->-r /content/requirements.txt (line 1)) (2021.11.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations->-r /content/requirements.txt (line 1)) (2.6.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/requirements.txt (line 9)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/requirements.txt (line 9)) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/requirements.txt (line 9)) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/requirements.txt (line 9)) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->-r /content/requirements.txt (line 9)) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations->-r /content/requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations->-r /content/requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r /content/requirements.txt (line 2)) (2022.2.1)\n",
            "Collecting pretrainedmodels==0.7.4\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 7.5 MB/s \n",
            "\u001b[?25hCollecting efficientnet-pytorch==0.7.1\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "Collecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[K     |████████████████████████████████| 376 kB 47.0 MB/s \n",
            "\u001b[?25hCollecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->-r /content/requirements.txt (line 14)) (2.23.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython->-r /content/requirements.txt (line 10)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->-r /content/requirements.txt (line 10)) (0.7.5)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 57.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from IPython->-r /content/requirements.txt (line 10)) (2.0.10)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->-r /content/requirements.txt (line 10)) (2.6.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from IPython->-r /content/requirements.txt (line 10)) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->-r /content/requirements.txt (line 10)) (5.1.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->-r /content/requirements.txt (line 10)) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython->-r /content/requirements.txt (line 10)) (57.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->IPython->-r /content/requirements.txt (line 10)) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython->-r /content/requirements.txt (line 10)) (0.2.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown->-r /content/requirements.txt (line 12)) (3.8.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown->-r /content/requirements.txt (line 12)) (4.6.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->IPython->-r /content/requirements.txt (line 10)) (0.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->-r /content/requirements.txt (line 14)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->-r /content/requirements.txt (line 14)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->-r /content/requirements.txt (line 14)) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->-r /content/requirements.txt (line 14)) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->-r /content/requirements.txt (line 14)) (1.7.1)\n",
            "Building wheels for collected packages: sklearn, efficientnet-pytorch, pretrainedmodels, GPUtil\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=38cbe36cd0ce42b127f3eaacba7303f5b98622ff64342d3816fdb68c7b413d5f\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=51cd804a67f9b2d4af88e4d439afc01e5e9a72991bb61c6053330a80db6f9980\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/cc/b2/49e74588263573ff778da58cc99b9c6349b496636a7e165be6\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60965 sha256=ac42bde4ea31d5de4fafe7df97a32d2039f312068a12b011259afe6a08734f23\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=bb2cb2a81bb6a7de7ebb0155529b70f4f13a6dbb13945ca7c172318641a0a1fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n",
            "Successfully built sklearn efficientnet-pytorch pretrainedmodels GPUtil\n",
            "Installing collected packages: munch, timm, pretrainedmodels, jedi, efficientnet-pytorch, sklearn, segmentation-models-pytorch, GPUtil\n",
            "Successfully installed GPUtil-1.4.0 efficientnet-pytorch-0.7.1 jedi-0.18.1 munch-2.5.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.0 sklearn-0.0 timm-0.4.12\n"
          ]
        }
      ],
      "source": [
        "!pip install -r /content/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Gzuv2run9Yxa"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import glob\n",
        "import multiprocessing\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.models import resnet18\n",
        "from torchvision.utils import draw_segmentation_masks\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import json\n",
        "from GPUtil import showUtilization as gpu_usage\n",
        "import GPUtil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5MpBxb2MjsO"
      },
      "source": [
        "# Датасет\n",
        "\n",
        "Прежде чем разбираться с моделями, нам надо в первую очередь разобраться с тем, как грузить датасет. Давайте напишем класс в торче для этого."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hPHj1jRrqDT"
      },
      "source": [
        "## Класс датасета"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IS7LtxhL8WWh"
      },
      "outputs": [],
      "source": [
        "class EyeDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Класс датасета, организующий загрузку и получение изображений и соответствующих разметок\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_folder: str, mode: str = \"train\", transform=None):\n",
        "        self.class_ids = {\"vessel\": 1}\n",
        "\n",
        "        self.mode = mode.lower()\n",
        "\n",
        "        self.data_folder = data_folder\n",
        "        self.transform = transform\n",
        "        self._image_files = glob.glob(f\"{data_folder}/*.png\")\n",
        "        if self.mode == \"train\":\n",
        "            self._mask_files = glob.glob(f\"{data_folder}/*.geojson\")\n",
        "\n",
        "        image_file_ids = set([el.split(\".\")[0] for el in self._image_files])\n",
        "        if self.mode == \"train\":\n",
        "            mask_file_ids = set([el.split(\".\")[0] for el in self._mask_files])\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "            intersecting_ids = list(image_file_ids.intersection(mask_file_ids))\n",
        "            self._image_files = [el + \".png\" for el in intersecting_ids]\n",
        "            self._mask_files = [el + \".geojson\" for el in intersecting_ids]\n",
        "\n",
        "    @staticmethod\n",
        "    def read_image(path: str) -> np.ndarray:\n",
        "        image = cv2.imread(str(path), cv2.IMREAD_COLOR)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = np.array(image / 255, dtype=np.float32)\n",
        "        return image\n",
        "\n",
        "    @staticmethod \n",
        "    def parse_polygon(coordinates, image_size): \n",
        "        mask = np.zeros(image_size, dtype=np.float32) \n",
        "    \n",
        "        if len(coordinates) == 1: \n",
        "            points = [np.int32(coordinates)] \n",
        "            cv2.fillPoly(mask, points, 1) \n",
        "        else: \n",
        "            points = [np.int32([coordinates[0]])] \n",
        "            cv2.fillPoly(mask, points, 1) \n",
        "    \n",
        "            for polygon in coordinates[1:]: \n",
        "                points = [np.int32([polygon])] \n",
        "                cv2.fillPoly(mask, points, 0) \n",
        "    \n",
        "        return mask\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_mask(shape: dict, image_size: tuple) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Метод для парсинга фигур из geojson файла\n",
        "        \"\"\"\n",
        "        mask = np.zeros(image_size, dtype=np.float32)\n",
        "        coordinates = shape['coordinates']\n",
        "        if shape['type'] == 'MultiPolygon':\n",
        "            for polygon in coordinates:\n",
        "                mask += EyeDataset.parse_polygon(polygon, image_size)\n",
        "        else:\n",
        "            mask += EyeDataset.parse_polygon(coordinates, image_size)\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def read_layout(self, path: str, image_size: tuple) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Метод для чтения geojson разметки и перевода в numpy маску\n",
        "        \"\"\"\n",
        "        with open(path, 'r', encoding='cp1251') as f:  # some files contain cyrillic letters, thus cp1251\n",
        "            json_contents = json.load(f)\n",
        "\n",
        "        num_channels = 1 + max(self.class_ids.values())\n",
        "        mask_channels = [np.zeros(image_size, dtype=np.float32) for _ in range(num_channels)]\n",
        "        mask = np.zeros(image_size, dtype=np.float32)\n",
        "\n",
        "        if type(json_contents) == dict and json_contents['type'] == 'FeatureCollection':\n",
        "            features = json_contents['features']\n",
        "        elif type(json_contents) == list:\n",
        "            features = json_contents\n",
        "        else:\n",
        "            features = [json_contents]\n",
        "\n",
        "        for shape in features:\n",
        "            channel_id = self.class_ids[\"vessel\"]\n",
        "            mask = self.parse_mask(shape['geometry'], image_size)\n",
        "            mask_channels[channel_id] = np.maximum(mask_channels[channel_id], mask)\n",
        "\n",
        "        mask_channels[0] = 1 - np.max(mask_channels[1:], axis=0)\n",
        "\n",
        "        return np.stack(mask_channels, axis=-1)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx: int) -> dict:\n",
        "        # Достаём имя файла по индексу\n",
        "        image_path = self._image_files[idx]\n",
        "\n",
        "        # Получаем соответствующий файл разметки\n",
        "        if self.mode == \"train\":\n",
        "            json_path = image_path.replace(\"png\", \"geojson\")\n",
        "\n",
        "        image = self.read_image(image_path)\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "            mask = self.read_layout(json_path, image.shape[:2])\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "            sample = {'image': image,\n",
        "                      'mask': mask}\n",
        "        else:\n",
        "            sample = {\"image\": image}\n",
        "\n",
        "        if self.transform is not None:\n",
        "            sample = self.transform(**sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._image_files)\n",
        "\n",
        "    # Метод для проверки состояния датасета\n",
        "    def make_report(self):\n",
        "      reports = []\n",
        "      if (not self.data_folder):\n",
        "        reports.append(\"Путь к датасету не указан\")\n",
        "      if (len(self._image_files) == 0):\n",
        "        reports.append(\"Изображения для распознавания не найдены\")\n",
        "      else:\n",
        "        reports.append(f\"Найдено {len(self._image_files)} изображений\")\n",
        "      cnt_images_without_masks = sum([1 - len(glob.glob(filepath.replace(\"png\", \"geojson\"))) for filepath in self._image_files])\n",
        "      if cnt_images_without_masks > 0:\n",
        "        reports.append(f\"Найдено {cnt_images_without_masks} изображений без разметки\")\n",
        "      else:\n",
        "        reports.append(f\"Для всех изображений есть файл разметки\")\n",
        "      return reports\n",
        "\n",
        "\n",
        "class DatasetPart(Dataset):\n",
        "    \"\"\"\n",
        "    Обертка над классом датасета для его разбиения на части\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset: Dataset,\n",
        "                 indices: np.ndarray,\n",
        "                 transform: A.Compose = None):\n",
        "        self.dataset = dataset\n",
        "        self.indices = indices\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx: int) -> dict:\n",
        "        sample = self.dataset[self.indices[idx]]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            sample = self.transform(**sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.indices)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msq76dphpPKj"
      },
      "source": [
        "# Предикшн"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5U9g7lMpPKl"
      },
      "source": [
        "## Класс Предиктора"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BvCJt1AbpPKm"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "class UnetPredictor:\n",
        "    def __init__(self, model: nn.Module, device: str):\n",
        "        self.model = model\n",
        "        self.device = torch.device(device)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def predict(self, dataloader: torch.utils.data.DataLoader) -> np.ndarray:\n",
        "        self.model.eval()\n",
        "\n",
        "        preds = []\n",
        "        print(len(dataloader))\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(dataloader):\n",
        "                image = batch[\"image\"].to(self.device)\n",
        "                output = self.model(image).cpu()\n",
        "                preds.append(torch.exp(output[:,1]))\n",
        "                del output\n",
        "                gc.collect()\n",
        "        return torch.cat(preds, dim=0).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqMtkry9pPKn"
      },
      "source": [
        "##Предсказание"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWw-4548i2ie",
        "outputId": "8063067c-8a66-4155-d857-822233ea28e8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BJp8ZTh4pPKo"
      },
      "outputs": [],
      "source": [
        "test_data = '/content/test/eye_test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SR2zN3M_qNEZ"
      },
      "outputs": [],
      "source": [
        "model = torch.load('/content/drive/MyDrive/Machine_Learning/Competitions/LeadersOfDigital/models/model.pt')\n",
        "#model_2 = torch.load('/content/drive/MyDrive/Competitions/LeadersOfDigital/models/model_1024px_10x_1438.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL42Fa_fpPKp",
        "outputId": "9142ba40-bfa4-4862-c1fe-4670f50ac127"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "301\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 301/301 [01:01<00:00,  4.87it/s]\n"
          ]
        }
      ],
      "source": [
        "cores=multiprocessing.cpu_count()\n",
        "size= 1024\n",
        "eval_aug = A.Compose([\n",
        "    A.LongestMaxSize(size, interpolation=cv2.INTER_CUBIC),\n",
        "    A.PadIfNeeded(size, size),\n",
        "    ToTensorV2(transpose_mask=True)\n",
        "])\n",
        "\n",
        "transforms = {\"test\": eval_aug}\n",
        "\n",
        "test_dataset = EyeDataset(\n",
        "    test_data, mode='predict', transform=transforms['test']\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                            batch_size=1,\n",
        "                                            num_workers=cores-1,\n",
        "                                            shuffle=False)\n",
        "\n",
        "predictor = UnetPredictor(model=model, device=\"cuda\")\n",
        "#predictor2 = UnetPredictor(model=model_2, device=\"cuda\")\n",
        "preds = predictor.predict(test_loader)\n",
        "#preds_2 = predictor2.predict(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "round_preds = preds"
      ],
      "metadata": {
        "id": "1nwOehzC3NYj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "treshhold = 0.4\n",
        "round_preds[round_preds>=treshhold]=255\n",
        "round_preds[round_preds<treshhold]=0"
      ],
      "metadata": {
        "id": "QTWl9yCgOr8s"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/preds"
      ],
      "metadata": {
        "id": "LH663tC0O2wH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MVqw2U_zpPKq"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "quality = 0.1\n",
        "box = (0, (size-777)/2, size, size-(size-777)/2)\n",
        "for i in range(len(round_preds)):\n",
        "    im = Image.fromarray(round_preds[i])\n",
        "    im = im.convert('1')\n",
        "    im = im.crop(box)\n",
        "    im = im.resize((1624,1232))\n",
        "    im.save(test_dataset._image_files[i].replace(\"/content/test/eye_test\", '/content/preds'),compress_level=9 ,bits=2, format=\"PNG\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -9 -q -r test_submit.zip /content/preds/"
      ],
      "metadata": {
        "id": "D-JI08O2Q_VZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhwMnrae4VZw",
        "outputId": "1c61fe2d-4cb1-45cc-c275-a9bc29f3a25c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ueMkZId_6w3T"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "l5MpBxb2MjsO",
        "3hPHj1jRrqDT"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}